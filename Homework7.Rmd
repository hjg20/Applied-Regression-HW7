---
title: 'STA 5207: Homework 7'
date: 'Due: March, 8th by 11:59 PM'
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Include your R code in an R chunks as part of your answer. In addition, your written answer to each exercise should be self-contained so that the grader can determine your solution without reading your code or deciphering its output.

## Exercise 1 (`longley` Macroeconomic Data) [50 points]

For this exercise we will use the built-in `longley` data set. You can also find the data in `longley.csv` on Canvas. The data set contains macroeconomic data for predicting unemployment. The variables in the model are

-   `GNP.deflator`: GNP implicit price deflator (1954 = 100)
-   `GNP`: Gross national product.
-   `Unemployed`: Number of unemployed.
-   `Armed.Forces`: Number of people in the armed forces.
-   `Population`: \`noninstituionalized population $\geq 14$ years of age.
-   `Year`: The year.
-   `Employed`: Number of people employed.

In the following exercise, we will model the `Employed` variable.

1.  (6 points) How many pairs of predictors are highly correlated? Consider "highly" correlated to be a sample correlation above 0.7. What is the largest correlation between any pair of predictors in the data set?

    ```{r}
    data <- longley
    preds <- dplyr::select(data, -Employed)
    round(cor(preds), 3)
    ```

    **Answer:** Highly correlated pairs of predictors (predictors with a sample correlation above 0.7) are [GNP and GNP.deflator]{.underline}, [Population and GNP.deflator]{.underline}, [Year and GNP.deflator]{.underline}, [Population and GNP]{.underline}, [Year and GNP]{.underline}, and [Year and Population]{.underline}. The largest correlation between any pair of predictors in the dataset is the correlation between Year and GNP with a correlation of .995.

2.  (6 points) Fit a model with `Employed` as the response and the remaining variables as predictors. Give the condition number. Does multicollinearity appear to be a problem

    ```{r}
    library(olsrr)
    model <- lm(Employed ~., data=data)
    summary(model)
    round(ols_eigen_cindex(model)[, 1:2], 4)
    ```

    **Answer:** The condition number is 43275.04. This is much greater than 30, so we say that multicollinearity appears to be a problem.

3.  (6 points) Calculate and report the variance inflation factor (VIF) for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

    ```{r}
    library(faraway)
    vif(model)
    ```

    **Answer:** The VIFs for each predictor are as follows: GNP.deflator = 135.5, GNP = 1788.5, Unemployed = 33.6, Armed.Forces = 3.6, Population = 399.2, Year = 759. The variable with the largest VIF is GNP with a VIF of 1788.5. A VIF greater than 5 suggests colinearity, so all predictor's except for Armed.Force's VIF values suggest multicollinearity.

4.  (6 points) What proportion of the observed variation in `Population` is explained by the linear relationship with the other predictors? Are there any variables that are nearly orthogonal to the others? Consider a low $R^2_k$ to be less than 0.3.

    ```{r}
    summary(lm(Population~GNP.deflator+GNP+Unemployed+Armed.Forces+Year, data=data))
    1-1/vif(model)
    ```

    **Answer:** The proportion of the observed variation in Population that is explained by the linear relationship with the other predictors is 99.75%. There are no variables that are nearly orthogonal to the others.

5.  (6 points) Give the condition indices. How many near linear-dependencies are likely causing most of the problem?

    ```{r}
    library(olsrr)
    round(ols_eigen_cindex(model), 3)
    ```

    **Answer:** There are 3 indexes in which the condition index is greater than 30. Therefore we can conclude that there are three linear-dependencies that are likely causing most of the problem.

6.  (10 points) Fit a new model with `Employed` as the the response and the predictors from the model in part 2 that were significant (use $\alpha = 0.05$). Calculate and report the variance inflation factor for each of the predictors. Do any of the VIFs suggest multicollinearity?

    ```{r}
    new_model <- lm(Employed ~ Unemployed + Armed.Forces + Year, data=data)
    vif(new_model)
    ```

    **Answer:** We choose Unemplyed, Armed.Forces, and Year as our predictors because they all had a p-value of less than 0.05 in the linear model from part 2. All VIFs are less than 5, so we conclude that collinearity is not a problem for this model.

7.  (10 points) Use an $F$-test to compare the models in parts 2 and 6. Report the following:

    -   The null hypothesis.
    -   The test statistic.
    -   The $p$-value of the test.
    -   A statistical decision at $\alpha = 0.05$.
    -   Which model do you prefer, the model from part 2 or 6.

    ```{r}
    anova(new_model, model)
    ```

    **Answer:** The null hypothesis is that the full model does not provide a significantly better fit to the data than the restricted model. The test-statistic for this test is 1.7465, and the p-value is 0.227. Since the p-value is not less than our significance level of 0.05, we do not reject the null hypothesis and thus conclude that the full model does not provide a significantly better fit to the data than the restricted model. Therefore, we would prefer the model from part 6 over the model from part 2.

## Exercise 2 (The `sat` Data Set Revisited) [50 points]

For this exercise we will use the `sat` data set from the `faraway` package, which you analyzed in Homework #3. In the following exercise, we will model the `total` variable as a function of `expend`, `salary`, and `ratio`.

1.  (8 points) Among the three predictors `expend`, `salary`, and \`ratio\`\`, how many pairs of predictors are are highly correlated? Consider "highly" correlated to be a sample correlation above 0.7.

    ```{r}
    library(faraway)
    data <- dplyr::select(data, total, expend, salary, ratio)
    preds <- dplyr::select(data, -total)
    round(cor(preds), 3)
    ```

    **Answer:** Using 0.7 as highly correlated, we see that the only pair of predictors which are highly correlated are salary and expend with a correlation score of 0.870.

2.  (8 points) Fit a model with `total` as the response and `expend`, `salary`, and `ratio` as the predictors. Give the condition number. Does multicollinearity appear to be a problem?

    ```{r}
    model <- lm(total ~ ., data=data)
    round(ols_eigen_cindex(model)[,1:2],4)
    ```

    **Answer:** The condition number is 48.1229. Since the condition number is greater than 30, we should be concerned about collinearity.

3.  (8 points) Calculate and report the variance inflation factor (VIF) for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

    ```{r}
    vif(model)
    ```

    **Answer:** The VIF values are as follows: expend = 9.39, salary = 8.10, ratio = 2.29. The variable with the largest VIF is expend. Expend and Salary both have VIF values greater than 5 which suggests collinearity.

4.  (10 points) Fit a new model with `total` as the response and `ratio` and the sum of `expend` and `salary` -- that is `I(expend + salary)` -- as the predictors. Note that `expend` and `salary` have the same units (thousands of dollars), so adding them makes sense. Calculate and report the variance inflation factor for each of the two predictors. Do any of the VIFs suggest multicollinearity?

    ```{r}
    new_model <- lm(total ~ ratio + I(expend + salary), data=data)
    vif(new_model)
    ```

    **Answer:** After adding expend and salary together, both of the sum of these predictors and ratio have a VIF value of 1.005151. Since this value is less than 5 we can say that non of the VIFs suggest multicollinearity.

5.  (6 points) Conduct a $t$-test at the 5% significance level for each slope parameter for the model in part 4. Give the test statistic, $p$-value, and statistical decision for each test.

    ```{r}
    summary(new_model)
    ```

    **Answer:** For the slope parameter corresponding to ratio, the test statistic is 0.382, the p-value is 0.704, and a statistical decision at the 5% significance level is to not reject the null hypothesis and say that there is no significant linear relationship between ratio and total with the other predictor present in the model. For the slope parameter corresponding to the sum of expend and salary, the test statistic is -3.305, the p-value is 0.00182, and a statistical decision at the 5% significance level is to reject the null hypothesis and say that there is a significant linear relationship between the sum of expend and salary and the response, total, with the other predictor present in the model.

6.  (10 points) Use an $F$-test to compare the models in parts 2 and 4. Report the following:

    -   The null hypothesis (**Hint**: We are testing a linear constraint, see the slides on MLR, page 39).
    -   The test statistic.
    -   The $p$-value of the test.
    -   A statistical decision at $\alpha = 0.05$.
    -   Which model do you prefer, the model from part 2 or part 4.

    ```{r}
    anova(new_model, model)
    ```

    **Answer:**
